{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec94f269-748e-4a95-b829-d519df6421a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1: Defining Helper Functions ---\n",
      "Helper functions defined successfully.\n",
      "\n",
      "--- Stage 2: Processing all source files... ---\n",
      "Individual file processing complete.\n",
      "\n",
      "--- Stage 3: Aggregating and unifying all data... ---\n",
      "Unification complete.\n",
      "\n",
      "--- Stage 4: Performing final cleanup and saving... ---\n",
      "\n",
      "--- Final Preprocessed Data (First 5 Rows) --- \n",
      "metric_type  YEAR    MONTH         CHANNEL  Spend  Total_Returns  Total_Revenue  Orders_Existing  Customers_Existing  Orders_New  Customers_New\n",
      "0            2024  JANUARY       Affiliate    0.0       -61116.9     1397391.57            116.0                 0.0         0.0            0.0\n",
      "1            2024  JANUARY          Direct    0.0       -61116.9     1397391.57            793.0                 0.0       800.0            0.0\n",
      "2            2024  JANUARY         Display    0.0       -61116.9     1397391.57              0.0                58.0         0.0           24.0\n",
      "3            2024  JANUARY           Email    0.0       -61116.9     1397391.57           1158.0                 0.0       271.0            0.0\n",
      "4            2024  JANUARY  Organic Search    0.0       -61116.9     1397391.57            382.0                 0.0       682.0            0.0\n",
      "\n",
      "Successfully saved the final, non-redundant data to 'final_unified_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# DTC Analytics: Final Data Preprocessing and Unification Script\n",
    "# ==============================================================================\n",
    "#\n",
    "# Objective: This script transforms the 13 raw source CSV files into a single,\n",
    "# clean, and non-redundant analytical dataset.\n",
    "#\n",
    "# Instructions:\n",
    "# 1. Ensure all 13 source CSV files are in the same directory as this script.\n",
    "# 2. Run the script to generate 'final_unified_dataset.csv'.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Stage 1: Setup and Helper Functions ---\n",
    "print(\"--- Stage 1: Defining Helper Functions ---\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress potential warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def find_header_row(df):\n",
    "    \"\"\"Dynamically finds the most likely header row in a dataframe.\"\"\"\n",
    "    for i, row in df.iterrows():\n",
    "        row_str = ' '.join(row.astype(str).str.upper().tolist())\n",
    "        if ('YEAR' in row_str or 'CHANNEL' in row_str or 'KPI' in row_str or 'MONTH' in row_str or 'RETURNS' in row_str) and row.notna().sum() > 2:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def clean_and_process_wide_format(file_path, metric_name, id_col_name='CHANNEL'):\n",
    "    \"\"\"Reads and processes wide-format files with months as columns.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None, dtype=str, encoding='latin-1')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Info: File '{file_path}' not found. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    header_row_index = find_header_row(df)\n",
    "    if header_row_index == -1:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    header = df.iloc[header_row_index].str.upper().str.strip()\n",
    "    data = df.iloc[header_row_index + 1:].copy()\n",
    "    data.columns = header\n",
    "\n",
    "    id_col = next((col for col in [id_col_name, 'KPI'] if col in data.columns), data.columns[0])\n",
    "    data = data.rename(columns={id_col: 'ID_COLUMN'})\n",
    "\n",
    "    if 'YEAR' not in data.columns:\n",
    "        return pd.DataFrame()\n",
    "    if isinstance(data['YEAR'], pd.DataFrame):\n",
    "        year_series = data['YEAR'].iloc[:, 0]\n",
    "    else:\n",
    "        year_series = data['YEAR']\n",
    "    data['YEAR'] = pd.to_numeric(year_series, errors='coerce')\n",
    "    data.dropna(subset=['YEAR'], inplace=True)\n",
    "    data['YEAR'] = data['YEAR'].astype(int)\n",
    "    \n",
    "    month_map = {'JANUARY': 1, 'FEBRUARY': 2, 'MARCH': 3, 'APRIL': 4, 'MAY': 5, 'JUNE': 6,\n",
    "                 'JULY': 7, 'AUGUST': 8, 'SEPTEMBER': 9, 'OCTOBER': 10, 'NOVEMBER': 11, 'DECEMBER': 12}\n",
    "    month_cols = [col for col in data.columns if col in month_map]\n",
    "    if not month_cols:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    melted = data.melt(id_vars=['ID_COLUMN', 'YEAR'], value_vars=month_cols, var_name='MONTH', value_name='value')\n",
    "    melted['value'] = pd.to_numeric(melted['value'].astype(str).str.replace(r'[$,()]', '', regex=True), errors='coerce')\n",
    "    melted.dropna(subset=['value'], inplace=True)\n",
    "    melted['Date'] = pd.to_datetime(melted['YEAR'].astype(str) + '-' + melted['MONTH'].map(month_map).astype(str) + '-01')\n",
    "    melted = melted.rename(columns={'ID_COLUMN': id_col_name})\n",
    "    melted['metric_type'] = metric_name\n",
    "    \n",
    "    return melted[['Date', id_col_name, 'value', 'metric_type']]\n",
    "\n",
    "print(\"Helper functions defined successfully.\")\n",
    "\n",
    "\n",
    "# --- Stage 2: Process Each Data Category ---\n",
    "print(\"\\n--- Stage 2: Processing all source files... ---\")\n",
    "\n",
    "file_processing_plan = [\n",
    "    {'path': 'Media Spend.csv', 'metric': 'Spend', 'id_col': 'CHANNEL'},\n",
    "    {'path': 'Media Spend by Channel.csv', 'metric': 'Spend', 'id_col': 'CHANNEL'},\n",
    "    {'path': 'Technology Spend.csv', 'metric': 'Technology_Spend', 'id_col': 'Vendor'},\n",
    "    {'path': 'Web Analytics.csv', 'metric': 'Sessions', 'id_col': 'CHANNEL'},\n",
    "    {'path': 'Marketing Channel Breakdown.csv', 'metric': 'Clicks', 'id_col': 'CHANNEL'},\n",
    "    {'path': 'Orders By Channel-New.csv', 'metric': 'Orders_New', 'id_col': 'CHANNEL'},\n",
    "    {'path': 'Orders By Channel-Ext.csv', 'metric': 'Orders_Existing', 'id_col': 'CHANNEL'},\n",
    "    {'path': 'Cust By Channel-New.csv', 'metric': 'Customers_New', 'id_col': 'CHANNEL'},\n",
    "    {'path': 'Cust By Channel-Ext.csv', 'metric': 'Customers_Existing', 'id_col': 'CHANNEL'},\n",
    "    {'path': 'TOPSHEET.csv', 'metric': 'Total_Revenue', 'id_col': 'KPI'}\n",
    "]\n",
    "\n",
    "all_dfs = []\n",
    "for plan in file_processing_plan:\n",
    "    df = clean_and_process_wide_format(plan['path'], plan['metric'], plan['id_col'])\n",
    "    if not df.empty:\n",
    "        df = df.rename(columns={plan['id_col']: 'ID'})\n",
    "        all_dfs.append(df)\n",
    "\n",
    "# Special handling for Returns.csv\n",
    "try:\n",
    "    returns_df = pd.read_csv('Returns.csv', header=0, dtype=str, encoding='latin-1')\n",
    "    date_col = next((col for col in returns_df.columns if 'MONTH' in col.upper()), None)\n",
    "    amount_col = next((col for col in returns_df.columns if 'RETURN' in col.upper()), None)\n",
    "    if date_col and amount_col:\n",
    "        returns_df['Date'] = pd.to_datetime(returns_df[date_col], errors='coerce')\n",
    "        returns_df['value'] = returns_df[amount_col].str.replace(r'[($),]', '', regex=True).astype(float) * -1\n",
    "        returns_monthly = returns_df[['Date', 'value']].dropna()\n",
    "        returns_monthly['metric_type'] = 'Total_Returns'\n",
    "        returns_monthly['ID'] = 'Business' # Assign a generic ID for business-level metrics\n",
    "        all_dfs.append(returns_monthly)\n",
    "except (FileNotFoundError, KeyError) as e:\n",
    "    print(f\"Info: Could not process 'Returns.csv'. Error: {e}. Skipping.\")\n",
    "\n",
    "print(\"Individual file processing complete.\")\n",
    "\n",
    "\n",
    "# --- Stage 3: Aggregation and Final Unification ---\n",
    "print(\"\\n--- Stage 3: Aggregating and unifying all data... ---\")\n",
    "from functools import reduce\n",
    "\n",
    "full_df = pd.concat(all_dfs)\n",
    "\n",
    "# Filter out summary rows and standardize channel names\n",
    "full_df = full_df[~full_df['ID'].str.contains('TOTAL', na=False)]\n",
    "full_df['ID'] = full_df['ID'].str.replace(' Media', '', regex=False).str.strip()\n",
    "\n",
    "# Separate business-level metrics from channel-level metrics\n",
    "business_metrics = ['Total_Revenue', 'Total_Returns', 'Technology_Spend']\n",
    "business_df = full_df[full_df['metric_type'].isin(business_metrics)]\n",
    "channel_df = full_df[~full_df['metric_type'].isin(business_metrics)]\n",
    "\n",
    "# Pivot the channel data to create the non-redundant structure\n",
    "channel_pivot = channel_df.pivot_table(\n",
    "    index=['Date', 'ID'], \n",
    "    columns='metric_type', \n",
    "    values='value', \n",
    "    aggfunc='sum'\n",
    ").reset_index().rename(columns={'ID': 'CHANNEL'})\n",
    "\n",
    "# Aggregate business data to a single value per date\n",
    "business_agg = business_df.groupby(['Date', 'metric_type'])['value'].sum().unstack().reset_index()\n",
    "\n",
    "# Merge the two dataframes together\n",
    "final_df = pd.merge(channel_pivot, business_agg, on='Date', how='left')\n",
    "\n",
    "print(\"Unification complete.\")\n",
    "\n",
    "\n",
    "# --- Stage 4: Final Cleanup and Saving ---\n",
    "print(\"\\n--- Stage 4: Performing final cleanup and saving... ---\")\n",
    "\n",
    "# Add YEAR and MONTH columns\n",
    "final_df['YEAR'] = final_df['Date'].dt.year\n",
    "final_df['MONTH'] = final_df['Date'].dt.month_name().str.upper()\n",
    "\n",
    "# Fill any remaining NaN values with 0\n",
    "numeric_cols = final_df.select_dtypes(include=np.number).columns\n",
    "final_df[numeric_cols] = final_df[numeric_cols].fillna(0)\n",
    "\n",
    "# Define final column order for clarity\n",
    "final_cols = ['YEAR', 'MONTH', 'CHANNEL', 'Sessions', 'Clicks', 'Spend', \n",
    "              'Total_Returns', 'Technology_Spend', 'Total_Revenue', \n",
    "              'Orders_Existing', 'Customers_Existing', 'Orders_New', 'Customers_New']\n",
    "existing_final_cols = [col for col in final_cols if col in final_df.columns]\n",
    "final_df = final_df[existing_final_cols]\n",
    "\n",
    "# Display final result sample\n",
    "print(\"\\n--- Final Preprocessed Data (First 5 Rows) --- \")\n",
    "print(final_df.head().to_string())\n",
    "\n",
    "# Save the final dataframe to a CSV file\n",
    "output_filename = 'final_unified_dataset.csv'\n",
    "final_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved the final, non-redundant data to '{output_filename}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32049db2-1fd2-4c82-bda1-969bae3491c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DTC Analytics)",
   "language": "python",
   "name": "dtc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
